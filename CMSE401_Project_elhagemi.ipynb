{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64762158",
   "metadata": {},
   "source": [
    "# Project Write-Up: Sorting Algorithm Performance Comparison\n",
    "\n",
    "In this project, we compared the performance of various sorting algorithms to determine the most efficient method for sorting large datasets. The algorithms we tested include Radix Sort, Quick Sort, Merge Sort, and their parallelized versions.\n",
    "\n",
    "## Dataset Generation\n",
    "\n",
    "We generated random datasets of varying sizes (100, 1,000, 10,000, 100,000, 1,000,000, and 10,000,000 elements) to test each algorithm's efficiency. The elements in the datasets were integers ranging from 1 to 100,000,000.\n",
    "\n",
    "## Sorting Algorithms\n",
    "\n",
    "We implemented the following sorting algorithms:\n",
    "\n",
    "1. Radix Sort\n",
    "2. Quick Sort\n",
    "3. Merge Sort\n",
    "4. Parallel Merge Sort\n",
    "5. Parallel Quick Sort\n",
    "6. Parallel Radix Sort\n",
    "\n",
    "## Performance Evaluation\n",
    "\n",
    "We measured the execution time of each algorithm using Python's `time` module. Each test was performed five times, and the average execution time was calculated for each dataset size. The results are presented in the table below.\n",
    "\n",
    "|                 Sort Type                | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| :--------------------------------------: | :----------: | :-----------: | :------------: | :-------------: | :--------------: | :---------------: |\n",
    "| Radix Sort                               |    0.0002s   |    0.0024s     |     0.03s      |      0.3s       |      4.616s      |     49.144s       |\n",
    "| Quick Sort                               |    0.00028s  |    0.00294s    |    0.031384s   |    0.680532s    |      8.66178s    |     59.76188s     |\n",
    "| Merge Sort                               |    0.00022s  |    0.003402s   |    0.038404s   |    0.4659s      |      5.313s      |     65.6322s      |\n",
    "| Parallel Merge Sort                      |    0.0312s   |    0.0336s     |    0.0522s     |     0.262s      |      2.842s      |     34.268s       |\n",
    "| Parallel Quick Sort                      |    0.0134s   |    0.0148s     |    0.0328s     |     0.242s      |      2.554s      |     36.78s        |\n",
    "| Parallel Radix Sort                      |    2.592s    |    2.522s      |    2.58s       |     2.842s      |      7.276s      |     49.812s       |\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "- Radix Sort performed well on larger datasets, while Quick Sort and Merge Sort performed better on smaller datasets.\n",
    "- Parallelizing the sorting algorithms significantly improved their performance, with Parallel Merge Sort emerging as the most efficient algorithm for datasets larger than 100 elements.\n",
    "- Parallel Quick Sort was also highly efficient, with performance comparable to Parallel Merge Sort, especially for larger datasets.\n",
    "- Parallel Radix Sort did not show a significant performance improvement compared to the non-parallelized version, and its performance was generally worse than the other parallel algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76716af1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b38108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f22d5",
   "metadata": {},
   "source": [
    "## Creating the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7dad6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_dataset(size, min_value, max_value):\n",
    "    return [random.randint(min_value, max_value) for _ in range(size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909145b8",
   "metadata": {},
   "source": [
    "## Radix Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23787fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radix Sort execution time: 0.4792642593383789 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(100, 1, 100000000)\n",
    "\n",
    "def counting_sort(arr, exp):\n",
    "    n = len(arr)\n",
    "    output = [0] * n\n",
    "    count = [0] * 10\n",
    "\n",
    "    for i in range(n):\n",
    "        index = arr[i] // exp\n",
    "        count[index % 10] += 1\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        count[i] += count[i - 1]\n",
    "\n",
    "    i = n - 1\n",
    "    while i >= 0:\n",
    "        index = arr[i] // exp\n",
    "        output[count[index % 10] - 1] = arr[i]\n",
    "        count[index % 10] -= 1\n",
    "        i -= 1\n",
    "\n",
    "    return output\n",
    "\n",
    "def radix_sort(arr):\n",
    "    if len(arr) == 0:\n",
    "        return arr\n",
    "\n",
    "    max_val = max(arr)\n",
    "    exp = 1\n",
    "    while max_val // exp > 0:\n",
    "        arr = counting_sort(arr, exp)\n",
    "        exp *= 10\n",
    "\n",
    "    return arr\n",
    "\n",
    "start_time = time.time()\n",
    "sorted_data_radix = radix_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Radix Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39d5ed",
   "metadata": {},
   "source": [
    "| Sort Type | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| --------- | ------------ | ------------- | -------------- | --------------- | ---------------- | ----------------- |\n",
    "| Radix Sort | 0.0002s <br> 0.0002s <br> 0.0003s <br> 0.0002s <br> 0.0003s <br> avg: 0.0002s | 0.002s <br> 0.003s <br> 0.003s <br> 0.003s <br> 0.001s <br> avg: 0.0024s | 0.03s <br> 0.03s <br> 0.03s <br> 0.03s <br> 0.03s <br> avg: 0.03s | 0.3s <br> 0.31s <br> 0.29s <br> 0.301s <br> 0.3s <br> avg: 0.3s | 6.02s <br> 4.94s <br> 3.9s <br> 4.01s <br> 4.31s <br> avg: 4.616s | 49.12s <br> 47.8s <br> 52.3s <br> 48.3 <br> 48.2 <br> avg: 49.144 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e327cb2",
   "metadata": {},
   "source": [
    "## Quick Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d7c7755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick Sort execution time: 0.00020623207092285156 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(100, 1, 100000000)\n",
    "\n",
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quick_sort(left) + middle + quick_sort(right)\n",
    "\n",
    "start_time = time.time()              \n",
    "sorted_data_quick = quick_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Quick Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d55ff",
   "metadata": {},
   "source": [
    "| Sort Type | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| --------- | ------------ | ------------- | -------------- | --------------- | ---------------- | ----------------- |\n",
    "| Quick Sort | 0.0002s <br> 0.0003s <br> 0.0004s <br> 0.0002s <br> 0.0003s <br> avg: 0.00028s | 0.0019s <br> 0.0032s <br> 0.0036s <br> 0.0037s <br> 0.0023s <br> avg: 0.00294s | 0.02332s <br> 0.024s <br> 0.0352s <br> 0.0391s <br> 0.0353s <br> avg: 0.031384s | 0.27116s <br> 0.6876s <br> 0.6395s <br> 1.1542s <br> 0.6502s <br> avg: 0.680532s | 6.2272s <br> 10.2s <br> 10.657s <br> 7.003s <br> 9.2217s <br> avg: 8.66178s | 59.292s <br> 63.4478s <br> 63.7266s <br> 54.233 <br> 58.11 <br> avg: 59.76188s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1103fd",
   "metadata": {},
   "source": [
    "## Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0b4c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Sort execution time: 0.0034945011138916016 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(1000, 1, 100000000)\n",
    "def merge(left, right):\n",
    "    result = []\n",
    "    i = j = 0\n",
    "\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "\n",
    "    result.extend(left[i:])\n",
    "    result.extend(right[j:])\n",
    "    return result\n",
    "\n",
    "def merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    mid = len(arr) // 2\n",
    "    left = arr[:mid]\n",
    "    right = arr[mid:]\n",
    "\n",
    "    left = merge_sort(left)\n",
    "    right = merge_sort(right)\n",
    "    return merge(left, right)\n",
    "\n",
    "start_time = time.time()              \n",
    "sorted_data_merge = merge_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Merge Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2ebf1",
   "metadata": {},
   "source": [
    "| Sort Type | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| --------- | ------------ | ------------- | -------------- | --------------- | ---------------- | ----------------- |\n",
    "| Merge Sort| 0.0002s <br> 0.0002s <br> 0.0003s <br> 0.0002s <br> 0.0002s <br> avg: 0.00022s | 0.00341s <br> 0.0041s <br> 0.003s <br> 0.0029s <br> 0.0036s <br> avg: 0.003402s | 0.03602s <br> 0.037s <br> 0.035s <br> 0.041s <br> 0.043s <br> avg: 0.038404s | 0.4295s <br> 0.51s <br> 0.39s <br> 0.53s <br> 0.47s <br> avg: 0.4659s | 5.365s <br> 4.92s <br> 5.78s <br> 6.11s <br> 4.39s <br> avg: 5.313s | 67.173s <br> 65.238s <br> 61.42s <br> 68.14s <br> 66.19s <br> avg: 65.6322s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483e19d",
   "metadata": {},
   "source": [
    "## Parallel Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7de3fbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Merge Sort execution time: 0.12523555755615234 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(100, 1, 100000000)\n",
    "\n",
    "def parallel_merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    mid = len(arr) // 2\n",
    "    left = arr[:mid]\n",
    "    right = arr[mid:]\n",
    "\n",
    "    # Parallelize the merge_sort calls\n",
    "    with multiprocessing.Pool(processes=2) as pool:\n",
    "        left, right = pool.map(merge_sort, (left, right))\n",
    "\n",
    "    return merge(left, right)\n",
    "\n",
    "start_time = time.time()\n",
    "sorted_data_parallel_merge = parallel_merge_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Parallel Merge Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a479b",
   "metadata": {},
   "source": [
    "## Parallel Quick Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5770bfd",
   "metadata": {},
   "source": [
    "|                 Sort Type                | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| :--------------------------------------: | :---------: | :-----------: | :------------: | :-------------: | :--------------: | :---------------: |\n",
    "| Parallel Merge Sort | 0.031s <br> 0.03s <br> 0.032s <br> 0.032s <br> 0.031s <br> avg: 0.0312s | 0.034s <br> 0.033s <br> 0.034s <br> 0.033s <br> 0.034s <br> avg: 0.0336s | 0.053s <br> 0.052s <br> 0.052s <br> 0.052s <br> 0.052s <br> avg: 0.0522s | 0.26s <br> 0.26s <br> 0.26s <br> 0.26s <br> 0.27s <br> avg: 0.262s | 2.81s <br> 2.82s <br> 2.91s <br> 2.82s <br> 2.85s <br> avg: 2.842s | 34.39s <br> 34.15s <br> 34.5s <br> 34.2s <br> 34.1s <br> avg: 34.268s |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc4ca5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Quick Sort execution time: 36.41708993911743 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(10000000, 1, 100000000)\n",
    "\n",
    "def parallel_quick_sort_task(arr):\n",
    "    return quick_sort(arr)\n",
    "\n",
    "def parallel_quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "\n",
    "    with multiprocessing.Pool(processes=2) as pool:\n",
    "        left, right = pool.map(parallel_quick_sort_task, (left, right))\n",
    "\n",
    "    return left + middle + right\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "sorted_data_parallel_quick = parallel_quick_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Parallel Quick Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787e633",
   "metadata": {},
   "source": [
    "|                  Sort Type                 | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| :---------------------------------------: | :---------: | :-----------: | :------------: | :-------------: | :--------------: | :---------------: |\n",
    "| Parallel Quick Sort  | 0.014s <br> 0.013s <br> 0.013s <br> 0.013s <br> 0.014s <br> avg: 0.0134s | 0.015s <br> 0.014s <br> 0.015s <br> 0.015s <br> 0.015s <br> avg: 0.0148s | 0.032s <br> 0.031s <br> 0.033s <br> 0.031s <br> 0.037s <br> avg: 0.0328s | 0.23s <br> 0.21s <br> 0.17s <br> 0.29s <br> 0.31s <br> avg: 0.242s | 2.24s <br> 2.39s <br> 2.16s <br> 3.94s <br> 2.04s <br> avg: 2.554s | 36.42s <br> 36.46s <br> 36.8s <br> 37.1s <br> 37.12s <br> avg: 36.78s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e2ca5",
   "metadata": {},
   "source": [
    "## Parallel Radix Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7e3ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Radix Sort execution time: 48.98061156272888 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_random_dataset(10000000, 1, 100000000)\n",
    "def parallel_radix_sort(arr):\n",
    "    if len(arr) == 0:\n",
    "        return arr\n",
    "\n",
    "    max_val = max(arr)\n",
    "    exp = 1\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        while max_val // exp > 0:\n",
    "            arr = pool.apply(counting_sort, args=(arr, exp))\n",
    "            exp *= 10\n",
    "\n",
    "    return arr\n",
    "\n",
    "start_time = time.time()\n",
    "sorted_data_parallel_radix = parallel_radix_sort(dataset)\n",
    "end_time = time.time()\n",
    "print(\"Parallel Radix Sort execution time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116dcc8",
   "metadata": {},
   "source": [
    "|                    Sort Type                   | 100 Elements | 1000 Elements | 10000 Elements | 100000 Elements | 1000000 Elements | 10000000 Elements |\n",
    "| :-------------------------------------------: | :---------: | :-----------: | :------------: | :-------------: | :--------------: | :---------------: |\n",
    "| Parallel Radix Sort | 2.62s <br> 2.52s <br> 2.53s <br> 2.66s <br> 2.63s <br> avg: 2.592s | 2.63s <br> 2.52s <br> 2.45s <br> 2.51s <br> 2.5s <br> avg: 2.522s | 2.6s <br> 2.56s <br> 2.56s <br> 2.58s <br> 2.6s <br> avg: 2.58s | 2.81s <br> 2.59s <br> 2.66s <br> 3.01s <br> 3.14s <br> avg: 2.842s | 6.81s <br> 7.63s <br> 7.25s <br> 7.49s <br> 7.81s <br> avg: 7.398s | 46.23s <br> 48.98s <br> 47.3s <br> 47.6s <br> 48.13s <br> avg: 47.648s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802becb",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f113b",
   "metadata": {},
   "source": [
    "Based on the performance data in the tables provided, we can draw the following conclusions:\n",
    "\n",
    "1. For smaller datasets (100 to 1000 elements), the differences in execution times among the sorting algorithms are not significant. However, as the dataset size increases, the performance differences become more apparent.\n",
    "2. Radix Sort, Quick Sort, and Merge Sort all perform well on larger datasets, but their parallelized versions show even better performance for datasets with 100,000 elements or more.\n",
    "3. Parallel Radix Sort's performance is not consistent with the other parallel algorithms, as its execution time is higher even for smaller datasets. This could be due to the overhead of parallelism in this particular implementation or inefficiencies in the code.\n",
    "\n",
    "In summary, for smaller datasets, the choice of sorting algorithm may not significantly impact performance. However, as the dataset size increases, parallelized algorithms such as Parallel Quick Sort and Parallel Merge Sort show improved performance compared to their non-parallel counterparts. Parallel Radix Sort could be further optimized, as its current performance is not on par with the other parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472bb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
